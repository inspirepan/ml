{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 200000\n",
    "\n",
    "zh_path = f\"data/zh_train_{data_size}.txt\"\n",
    "en_path = f\"data/en_train_{data_size}.txt\"\n",
    "\n",
    "zh_data = []\n",
    "en_data = []\n",
    "\n",
    "if not os.path.exists(zh_path) or not os.path.exists(en_path):\n",
    "    zh_en_data = load_dataset(\"wmt19\", \"zh-en\", split=f\"train[:{data_size}]\")\n",
    "    print(zh_en_data.shape)\n",
    "    print(zh_en_data.cache_files)\n",
    "    with open(zh_path, \"w\") as f:\n",
    "        for i in zh_en_data:\n",
    "            zh_text = i['translation']['zh']\n",
    "            f.write(zh_text + \"\\n\")\n",
    "            zh_data.append(zh_text)\n",
    "    with open(en_path, \"w\") as f:\n",
    "        for i in zh_en_data:\n",
    "            en_text = i['translation']['en']\n",
    "            f.write(en_text + \"\\n\")\n",
    "            en_data.append(en_text)\n",
    "else:\n",
    "    with open(zh_path, \"r\") as f:\n",
    "        zh_data = f.readlines()\n",
    "    with open(en_path, \"r\") as f:\n",
    "        en_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.542 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 193330\n",
      "max zh len: 52\n",
      "max en len: 52\n",
      "en_vocab_size: 32310\n",
      "zh_vocab_size: 44129\n",
      "train len: 173997\n",
      "test len: 19333\n"
     ]
    }
   ],
   "source": [
    "filter_max_len = 50\n",
    "unk_freq = 2\n",
    "\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    text = text.replace(\"，\", \",\").replace(\"。\", \".\").replace(\"！\", \"!\").replace(\"？\", \"?\")\n",
    "    text = text.replace(\"[\", \"(\").replace(\"]\", \")\").replace(\"（\", \"(\").replace(\"）\", \")\").replace(\" \", \"\")\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9,.!?()]+', '', text)\n",
    "    text = text.lower()\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r\"\\w+|[\\.\\,\\?\\!\\(\\)]\", text)\n",
    "\n",
    "\n",
    "def build_vocabs(data_tokenized):\n",
    "    vocabs = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "    vocab_freq = defaultdict(int)\n",
    "    for seq in data_tokenized:\n",
    "        for v in seq:\n",
    "            vocab_freq[v] += 1\n",
    "\n",
    "    for v, freq in vocab_freq.items():\n",
    "        if freq >= unk_freq:\n",
    "            vocabs.append(v)\n",
    "\n",
    "    stoi = {s: i for i, s in enumerate(vocabs)}\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    vocab_size = len(stoi)\n",
    "\n",
    "    return stoi, itos, vocab_size, vocab_freq\n",
    "\n",
    "\n",
    "def encode(seq, stoi, add_sos=True, add_eos=True):\n",
    "    indices = [stoi[c] if c in stoi else stoi['<unk>'] for c in seq]\n",
    "    if add_sos:\n",
    "        indices = [stoi['<sos>']] + indices\n",
    "    if add_eos:\n",
    "        indices = indices + [stoi['<eos>']]\n",
    "    return indices\n",
    "\n",
    "\n",
    "def decode(indices, itos):\n",
    "    def decode_token(t):\n",
    "        if isinstance(t, int) and t != -1:\n",
    "            return itos[t]\n",
    "        elif isinstance(t, torch.Tensor) and t.item() != -1:\n",
    "            return itos[t.item()]\n",
    "        else:\n",
    "            return 'X'\n",
    "    result = [decode_token(idx) for idx in indices]\n",
    "    return \"|\".join(result)\n",
    "\n",
    "\n",
    "def build_dataset(tokenized):\n",
    "    size = len(tokenized)\n",
    "    xs = torch.zeros((size, max_en_len), dtype=torch.long)\n",
    "    ys = torch.zeros((size, max_zh_len), dtype=torch.long)\n",
    "\n",
    "    for i, content in enumerate(tokenized):\n",
    "        en_encoded = encode(content[0], en_stoi)\n",
    "        zh_encoded = encode(content[1], zh_stoi)\n",
    "        xs[i, :len(en_encoded)] = torch.tensor(en_encoded)\n",
    "        ys[i, :len(zh_encoded)] = torch.tensor(zh_encoded)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "zh_tokenized = []\n",
    "en_tokenized = []\n",
    "max_zh_len = 0\n",
    "max_en_len = 0\n",
    "\n",
    "\n",
    "for text_zh, text_en in zip(zh_data, en_data):\n",
    "    zh = tokenize_zh(text_zh)\n",
    "    en = tokenize_en(text_en)\n",
    "    if len(zh) > filter_max_len or len(en) > filter_max_len:\n",
    "        continue\n",
    "\n",
    "    zh_tokenized.append(zh)\n",
    "    en_tokenized.append(en)\n",
    "    if len(zh) > max_zh_len:\n",
    "        max_zh_len = len(zh)\n",
    "    if len(en) > max_en_len:\n",
    "        max_en_len = len(en)\n",
    "\n",
    "print(\"data length:\", len(zh_tokenized))\n",
    "\n",
    "max_zh_len += 2  # sos eos\n",
    "max_en_len += 2  # sos eos\n",
    "print(\"max zh len:\", max_zh_len)\n",
    "print(\"max en len:\", max_en_len)\n",
    "max_len = max(max_zh_len, max_en_len)\n",
    "\n",
    "en_stoi, en_itos, en_vocab_size, en_freq = build_vocabs(en_tokenized)\n",
    "zh_stoi, zh_itos, zh_vocab_size, zh_freq = build_vocabs(zh_tokenized)\n",
    "print(\"en_vocab_size:\", en_vocab_size)\n",
    "print(\"zh_vocab_size:\", zh_vocab_size)\n",
    "\n",
    "\n",
    "tokenized = list(zip(en_tokenized, zh_tokenized))\n",
    "xs, ys = build_dataset(tokenized)\n",
    "dataset = TensorDataset(xs, ys)\n",
    "g = torch.Generator().manual_seed(42)\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size], generator=g)\n",
    "print(\"train len:\", len(train_set))\n",
    "print(\"test len:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, masking=True):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size)\n",
    "        self.key = nn.Linear(n_embd, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size)\n",
    "        self.masking = masking\n",
    "        if masking:\n",
    "            self.register_buffer(\"mask\", torch.tril(torch.ones(2048, 2048)))\n",
    "\n",
    "    def forward(self, x, encoder_output=None, src_padding_mask=None):\n",
    "        # x: (B, T, n_embd)\n",
    "        # encoder_output: (B, T1, n_embd)\n",
    "        # src_padding_mask: (B, T)\n",
    "\n",
    "        B, T = x.shape[0], x.shape[1]\n",
    "\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        if encoder_output is not None:\n",
    "            # CROSS-ATTENTION\n",
    "            # encoder_output: (B, T1, n_embd)\n",
    "            k = self.key(encoder_output)  # (B, T1, head_size)\n",
    "            v = self.value(encoder_output)  # (B, T1, head_size)\n",
    "        else:\n",
    "            # SELF-ATTENTION\n",
    "            k = self.key(x)\n",
    "            v = self.value(x)\n",
    "        attn = q @ k.transpose(-2, -1)  # (B, T, T) or (B, T, T1)\n",
    "        attn = attn * (k.size(-1) ** -0.5)\n",
    "        if self.masking:\n",
    "            attn = attn.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        if src_padding_mask is not None:\n",
    "            if encoder_output is not None:\n",
    "                attn = attn.masked_fill(src_padding_mask.unsqueeze(1), float('-inf'))\n",
    "            else:\n",
    "                attn = attn.masked_fill(src_padding_mask.unsqueeze(1).expand(-1, T, -1), float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn, dim=-1)  # (B, T, T)\n",
    "\n",
    "        out = attn_weights @ v  # (B, T, head_size)\n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, masking=True):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, masking) for _ in range(n_head)])\n",
    "        self.fc = nn.Linear(n_head * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, encoder_output=None, src_padding_mask=None, return_attn=False):\n",
    "        attn_outs = [head(x, encoder_output, src_padding_mask) for head in self.heads]\n",
    "        attns = [out[0] for out in attn_outs]\n",
    "        out = torch.cat(attns, dim=-1)  # (B, T, n_head * head_size)\n",
    "        out = self.dropout(self.fc(out))  # (B, T, n_embd)\n",
    "        if return_attn:\n",
    "            attn_weights = [out[1] for out in attn_outs]\n",
    "            return out, attn_weights\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHead(n_embd, head_size, n_head, masking=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x, src_padding_mask=None, return_attn=False):\n",
    "        if return_attn:\n",
    "            sa_out, attn_weights = self.sa(self.ln1(x), src_padding_mask=src_padding_mask, return_attn=True)\n",
    "        else:\n",
    "            sa_out = self.sa(self.ln1(x), src_padding_mask=src_padding_mask)\n",
    "        x = x + sa_out\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return (x, attn_weights) if return_attn else x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHead(n_embd, head_size, n_head, masking=True)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ca = MultiHead(n_embd, head_size, n_head, masking=False)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_padding_mask=None, return_attn=False):\n",
    "        # x: (B, T, n_embd)\n",
    "        # encoder_output: (B, T1, n_embd)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        if return_attn:\n",
    "            ca_out, attn_weights = self.ca(self.ln2(x), encoder_output, src_padding_mask, return_attn=True)\n",
    "        else:\n",
    "            ca_out = self.ca(self.ln2(x), encoder_output, src_padding_mask)\n",
    "            attn_weights = None\n",
    "        x = x + ca_out\n",
    "        x = x + self.ff(self.ln3(x))\n",
    "        return (x, attn_weights) if return_attn else x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, n_layers, src_vocab_size, tgt_vocab_size, max_len, dropout=0.3):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, n_embd)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(max_len, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(n_embd, head_size, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(n_embd, head_size, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.ln_in = nn.LayerNorm(n_embd)\n",
    "        self.ln_tgt_in = nn.LayerNorm(n_embd)\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def encode(self, src, return_attn=False):\n",
    "        B, T1 = src.shape\n",
    "        src_padding_mask = (src == 0).to(src.device)  # (B, T1)\n",
    "\n",
    "        # ENCODER\n",
    "        tok_emb = self.src_embedding(src)  # (B, T1, n_embd)\n",
    "        pos = torch.arange(0, T1, device=src.device).unsqueeze(0).repeat(B, 1)  # (B, T1)\n",
    "        pos_emb = self.pos_embedding(pos)  # (B, T1, n_embd)\n",
    "        x = self.ln_in(tok_emb + pos_emb)  # (B, T1, n_embd)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        self_attns = []\n",
    "        # Pass padding mask to each encoder block\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            if return_attn:\n",
    "                x, attn_weights = encoder_block(x, src_padding_mask, return_attn=True)\n",
    "                self_attns.append(attn_weights)\n",
    "            else:\n",
    "                x = encoder_block(x, src_padding_mask)\n",
    "        encoder_output = self.ln_f(x)  # (B, T1, n_embd)\n",
    "        return (encoder_output, src_padding_mask, self_attns) if return_attn else (encoder_output, src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, encoder_output, src_padding_mask, return_attn=False):\n",
    "        B, T2 = tgt.shape\n",
    "        tgt_tok_emb = self.tgt_embedding(tgt)  # (B, T2, n_embd)\n",
    "        pos = torch.arange(0, T2, device=tgt.device).unsqueeze(0).repeat(B, 1)  # (B, T2)\n",
    "        pos_emb = self.pos_embedding(pos)  # (B, T2, n_embd)\n",
    "        x = self.ln_tgt_in(tgt_tok_emb + pos_emb)  # (B, T2, n_embd)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        cross_attns = []\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            if return_attn:\n",
    "                x, attn_weights = decoder_block(x, encoder_output, src_padding_mask, return_attn=True)\n",
    "                cross_attns.append(attn_weights)\n",
    "            else:\n",
    "                x = decoder_block(x, encoder_output, src_padding_mask)\n",
    "        x = self.ln_f(x)  # (B, T2, n_embd)\n",
    "        logits = F.linear(x, self.tgt_embedding.weight)  # (B, T2, tgt_vocab_size)\n",
    "        return (logits, cross_attns) if return_attn else logits\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: (B, T1), tgt: (B, T2)\n",
    "        B = src.shape[0]\n",
    "        encoder_output, src_padding_mask = self.encode(src)\n",
    "        decoder_target = torch.cat([tgt[:, 1:], torch.zeros(B, 1, dtype=tgt.dtype, device=tgt.device)], dim=1)\n",
    "        logits = self.decode(tgt, encoder_output, src_padding_mask)  # (B, T2, tgt_vocab_size)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), decoder_target.view(-1), ignore_index=0)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.improvement_counter = 0\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score >= self.best_score - self.min_delta:\n",
    "            self.improvement_counter = 0\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.improvement_counter += 1\n",
    "            if self.improvement_counter >= 3:\n",
    "                self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_length: 173997, test_length: 19333, batch_size: 128, filter_max_len: 50, unk_freq: 2\n",
      "lr: 0.001, weight_decay: 0.01\n",
      "n_embd: 128, n_head: 4, n_layers: 6, src_vocab_size: 32310, tgt_vocab_size: 44129, max_len: 52, dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layers = 6\n",
    "dropout = 0.3\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "print(f\"train_length: {len(train_set)}, test_length: {len(test_set)}, batch_size: {batch_size}, filter_max_len: {filter_max_len}, unk_freq: {unk_freq}\")\n",
    "print(f\"lr: {lr}, weight_decay: {weight_decay}\")\n",
    "print(f\"n_embd: {n_embd}, n_head: {n_head}, n_layers: {n_layers}, src_vocab_size: {en_vocab_size}, tgt_vocab_size: {zh_vocab_size}, max_len: {max(max_en_len, max_zh_len)}, dropout: {dropout}\")\n",
    "\n",
    "model = Model(n_embd, n_head, n_layers, en_vocab_size, zh_vocab_size, max(max_en_len, max_zh_len), dropout)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, test_loss, path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'en_stoi': en_stoi,\n",
    "        'en_itos': en_itos,\n",
    "        'zh_stoi': zh_stoi,\n",
    "        'zh_itos': zh_itos,\n",
    "        'model_config': {\n",
    "            'n_embd': n_embd,\n",
    "            'n_head': n_head,\n",
    "            'n_layers': n_layers,\n",
    "            'src_vocab_size': en_vocab_size,\n",
    "            'tgt_vocab_size': zh_vocab_size,\n",
    "            'max_len': max(max_en_len, max_zh_len),\n",
    "            'dropout': dropout\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    # 重建模型\n",
    "    model = Model(**checkpoint['model_config'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将模型状态移动到正确的设备\n",
    "    if device.type == 'cuda':\n",
    "        checkpoint['model_state_dict'] = {k: v.to(device) for k, v in checkpoint['model_state_dict'].items()}\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    en_stoi = checkpoint['en_stoi']\n",
    "    en_itos = checkpoint['en_itos']\n",
    "    zh_stoi = checkpoint['zh_stoi']\n",
    "    zh_itos = checkpoint['zh_itos']\n",
    "    max_len = checkpoint['model_config']['max_len']\n",
    "\n",
    "    return model, checkpoint, en_stoi, en_itos, zh_stoi, zh_itos, max_len\n",
    "\n",
    "\n",
    "def print_progress(epoch, progress, loss, epoch_time):\n",
    "    bar_length = 30  # Length of the progress bar\n",
    "    filled_length = int(bar_length * progress // 100)\n",
    "    bar = '━' * filled_length + '─' * (bar_length - filled_length)\n",
    "    time_str = f\"{epoch_time:.1f}s\" if epoch_time < 60 else f\"{epoch_time/60:.1f}min\"\n",
    "    print(f\"epoch {epoch}, progress |{bar}| {progress:.0f}%, loss {loss.item():.4f}, time: {time_str}\", end='\\r')\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            with autocast(device.type):\n",
    "                logits, loss = model(x, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_samples += x.size(0)\n",
    "    model.train()\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, avg train loss: 8.9091, test loss: 6.7136, lr: 0.00100, epoch time: 2.8min, total time: 2.8min\n",
      "epoch 1, avg train loss: 6.6046, test loss: 6.2527, lr: 0.00100, epoch time: 2.8min, total time: 5.6min\n",
      "epoch 2, avg train loss: 6.1442, test loss: 5.7623, lr: 0.00100, epoch time: 2.9min, total time: 8.6min\n",
      "epoch 3, avg train loss: 5.7105, test loss: 5.3320, lr: 0.00100, epoch time: 2.8min, total time: 11.3min\n",
      "epoch 4, avg train loss: 5.3305, test loss: 4.9131, lr: 0.00100, epoch time: 2.9min, total time: 14.3min\n",
      "epoch 5, avg train loss: 4.9941, test loss: 4.5986, lr: 0.00100, epoch time: 2.8min, total time: 17.1min\n",
      "epoch 6, avg train loss: 4.7104, test loss: 4.3386, lr: 0.00100, epoch time: 2.9min, total time: 20.0min\n",
      "epoch 7, avg train loss: 4.4719, test loss: 4.1262, lr: 0.00100, epoch time: 2.8min, total time: 22.8min\n",
      "epoch 8, avg train loss: 4.2692, test loss: 3.9199, lr: 0.00100, epoch time: 2.9min, total time: 25.7min\n",
      "epoch 9, avg train loss: 4.0939, test loss: 3.7685, lr: 0.00100, epoch time: 2.9min, total time: 28.5min\n",
      "检查点已保存到: checkpoint/transformer_checkpoint_epoch_10.pt\n",
      "epoch 10, avg train loss: 3.9421, test loss: 3.6655, lr: 0.00100, epoch time: 2.8min, total time: 31.4min\n",
      "epoch 11, avg train loss: 3.8098, test loss: 3.5481, lr: 0.00100, epoch time: 2.9min, total time: 34.3min\n",
      "epoch 12, avg train loss: 3.6930, test loss: 3.4566, lr: 0.00100, epoch time: 2.8min, total time: 37.2min\n",
      "epoch 13, avg train loss: 3.5876, test loss: 3.3696, lr: 0.00100, epoch time: 2.8min, total time: 40.0min\n",
      "epoch 14, avg train loss: 3.4935, test loss: 3.2977, lr: 0.00100, epoch time: 2.9min, total time: 42.8min\n",
      "epoch 15, avg train loss: 3.4078, test loss: 3.2453, lr: 0.00100, epoch time: 2.8min, total time: 45.7min\n",
      "epoch 16, avg train loss: 3.3290, test loss: 3.1774, lr: 0.00100, epoch time: 2.9min, total time: 48.5min\n",
      "epoch 17, avg train loss: 3.2563, test loss: 3.1343, lr: 0.00100, epoch time: 2.9min, total time: 51.4min\n",
      "epoch 18, avg train loss: 3.1899, test loss: 3.0820, lr: 0.00100, epoch time: 2.9min, total time: 54.3min\n",
      "epoch 19, avg train loss: 3.1287, test loss: 3.0405, lr: 0.00100, epoch time: 2.9min, total time: 57.1min\n",
      "检查点已保存到: checkpoint/transformer_checkpoint_epoch_20.pt\n",
      "epoch 20, avg train loss: 3.0704, test loss: 2.9970, lr: 0.00100, epoch time: 2.8min, total time: 60.0min\n",
      "epoch 21, avg train loss: 3.0168, test loss: 2.9645, lr: 0.00100, epoch time: 2.9min, total time: 62.9min\n",
      "epoch 22, avg train loss: 2.9687, test loss: 2.9311, lr: 0.00100, epoch time: 2.8min, total time: 65.7min\n",
      "epoch 23, avg train loss: 2.9219, test loss: 2.8944, lr: 0.00100, epoch time: 2.9min, total time: 68.5min\n",
      "epoch 24, avg train loss: 2.8792, test loss: 2.8712, lr: 0.00100, epoch time: 2.8min, total time: 71.3min\n",
      "epoch 25, avg train loss: 2.8375, test loss: 2.8487, lr: 0.00100, epoch time: 2.9min, total time: 74.2min\n",
      "epoch 26, avg train loss: 2.7992, test loss: 2.8204, lr: 0.00100, epoch time: 2.8min, total time: 77.0min\n",
      "epoch 27, avg train loss: 2.7639, test loss: 2.8075, lr: 0.00100, epoch time: 2.9min, total time: 79.9min\n",
      "epoch 28, avg train loss: 2.7294, test loss: 2.7836, lr: 0.00100, epoch time: 2.8min, total time: 82.8min\n",
      "epoch 29, avg train loss: 2.6969, test loss: 2.7620, lr: 0.00100, epoch time: 2.9min, total time: 85.7min\n",
      "检查点已保存到: checkpoint/transformer_checkpoint_epoch_30.pt\n",
      "epoch 30, avg train loss: 2.6667, test loss: 2.7408, lr: 0.00100, epoch time: 2.8min, total time: 88.5min\n",
      "epoch 31, avg train loss: 2.6376, test loss: 2.7449, lr: 0.00100, epoch time: 2.9min, total time: 91.4min\n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 32, avg train loss: 2.6103, test loss: 2.7125, lr: 0.00100, epoch time: 2.8min, total time: 94.2min\n",
      "epoch 33, avg train loss: 2.5823, test loss: 2.7000, lr: 0.00100, epoch time: 2.8min, total time: 97.1min\n",
      "epoch 34, avg train loss: 2.5581, test loss: 2.6894, lr: 0.00100, epoch time: 2.9min, total time: 100.0min\n",
      "epoch 35, avg train loss: 2.5345, test loss: 2.6714, lr: 0.00100, epoch time: 2.8min, total time: 102.8min\n",
      "epoch 36, avg train loss: 2.5120, test loss: 2.6679, lr: 0.00100, epoch time: 2.8min, total time: 105.6min\n",
      "epoch 37, avg train loss: 2.4899, test loss: 2.6625, lr: 0.00100, epoch time: 3.0min, total time: 108.6min\n",
      "epoch 38, avg train loss: 2.4689, test loss: 2.6538, lr: 0.00100, epoch time: 2.9min, total time: 111.5min\n",
      "epoch 39, avg train loss: 2.4492, test loss: 2.6448, lr: 0.00100, epoch time: 2.9min, total time: 114.4min\n",
      "检查点已保存到: checkpoint/transformer_checkpoint_epoch_40.pt\n",
      "epoch 40, avg train loss: 2.4296, test loss: 2.6366, lr: 0.00100, epoch time: 2.9min, total time: 117.3min\n",
      "epoch 41, avg train loss: 2.4119, test loss: 2.6260, lr: 0.00100, epoch time: 2.8min, total time: 120.2min\n",
      "epoch 42, avg train loss: 2.3939, test loss: 2.6063, lr: 0.00100, epoch time: 2.9min, total time: 123.1min\n",
      "epoch 43, avg train loss: 2.3764, test loss: 2.6159, lr: 0.00100, epoch time: 2.8min, total time: 125.9min\n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 44, avg train loss: 2.3608, test loss: 2.6153, lr: 0.00050, epoch time: 2.9min, total time: 128.8min\n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch 45, avg train loss: 2.2862, test loss: 2.5784, lr: 0.00050, epoch time: 2.8min, total time: 131.5min\n",
      "epoch 46, avg train loss: 2.2638, test loss: 2.5767, lr: 0.00050, epoch time: 2.9min, total time: 134.5min\n",
      "epoch 47, avg train loss: 2.2520, test loss: 2.5685, lr: 0.00050, epoch time: 2.8min, total time: 137.3min\n",
      "epoch 48, avg train loss: 2.2393, test loss: 2.5662, lr: 0.00050, epoch time: 2.9min, total time: 140.1min\n",
      "epoch 49, avg train loss: 2.2316, test loss: 2.5616, lr: 0.00050, epoch time: 2.8min, total time: 143.0min\n",
      "检查点已保存到: checkpoint/transformer_checkpoint_epoch_50.pt\n",
      "epoch 50, avg train loss: 2.2219, test loss: 2.5596, lr: 0.00050, epoch time: 2.9min, total time: 145.9min\n",
      "epoch 51, avg train loss: 2.2120, test loss: 2.5648, lr: 0.00050, epoch time: 2.8min, total time: 148.7min\n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 52, avg train loss: 2.2051, test loss: 2.5595, lr: 0.00025, epoch time: 2.9min, total time: 151.7min\n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch 53, avg train loss: 2.1651, test loss: 2.5420, lr: 0.00025, epoch time: 2.9min, total time: 154.5min\n",
      "epoch 54, avg train loss: 2.1551, test loss: 2.5387, lr: 0.00025, epoch time: 2.9min, total time: 157.4min\n",
      "epoch 55, avg train loss: 2.1478, test loss: 2.5460, lr: 0.00025, epoch time: 2.9min, total time: 160.3min\n",
      "EarlyStopping counter: 3 out of 5\n",
      "epoch 56, avg train loss: 2.1426, test loss: 2.5453, lr: 0.00013, epoch time: 2.9min, total time: 163.2min\n",
      "EarlyStopping counter: 4 out of 5\n",
      "epoch 57, avg train loss: 2.1218, test loss: 2.5399, lr: 0.00013, epoch time: 2.8min, total time: 166.0min\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered. Training has been stopped.\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001, verbose=True)\n",
    "\n",
    "while epoch < max_epoch:\n",
    "    epoch_start_time = time.time()  # 记录每个epoch开始时间\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        with autocast(device.type):\n",
    "            out, loss = model(x, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses.append(loss.item())\n",
    "        if i % (len(train_loader) // 20) == 0 and i > 0:\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            print_progress(epoch, (i + 1) / len(train_loader) * 100, loss, epoch_time)\n",
    "\n",
    "    test_loss = evaluate(model, test_loader, device)\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)  # Calculate average training loss\n",
    "    epoch_time = time.time() - epoch_start_time  # 计算每个epoch耗时\n",
    "    total_time = time.time() - start_time  # 计算总耗时\n",
    "    print(\" \" * 90, end=\"\\r\")\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # 打印训练信息，包含时间\n",
    "    epoch_time_str = f\"{epoch_time:.1f}s\" if epoch_time < 60 else f\"{epoch_time/60:.1f}min\"\n",
    "    total_time_str = f\"{total_time:.1f}s\" if total_time < 60 else f\"{total_time/60:.1f}min\"\n",
    "    print(f\"epoch {epoch}, avg train loss: {avg_train_loss:.4f}, test loss: {test_loss:.4f}, \"\n",
    "          f\"lr: {scheduler.get_last_lr()[0]:.5f}, epoch time: {epoch_time_str}, total time: {total_time_str}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_path = f\"checkpoint/transformer_checkpoint_epoch_{epoch+1}.pt\"\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, test_loss, save_path)\n",
    "        print(f\"检查点已保存到: {save_path}\")\n",
    "\n",
    "    early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Training has been stopped.\")\n",
    "        break\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型已保存到: checkpoint/transformer_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"checkpoint/transformer_checkpoint.pt\"\n",
    "save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, test_loss, save_path)\n",
    "print(f\"\\n模型已保存到: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(model, src, sos_token, eos_token, max_len=50, device='cpu', top_k=5, temperature=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = src.to(device)\n",
    "        B, T = src.shape[0], src.shape[1]\n",
    "        encoder_output, src_padding_mask = model.encode(src)\n",
    "        sequences = torch.ones(B, 1, dtype=torch.long, device=device) * sos_token\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = model.decode(sequences, encoder_output, src_padding_mask)\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "            topk_logits, top_k_indices = torch.topk(logits, k=min(top_k, logits.size(-1)))  # (B, k)\n",
    "            probs = F.softmax(topk_logits, dim=-1)  # (B, k)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = torch.gather(top_k_indices, 1, next_token_idx)\n",
    "            sequences = torch.cat([sequences, next_token], dim=1)\n",
    "            if (next_token == eos_token).all():\n",
    "                break\n",
    "    model.train()\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def beam_search(model, src, sos_token, eos_token, beam_width=5, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = src.to(device)\n",
    "        B, T = src.shape[0], src.shape[1]\n",
    "        encoder_output, src_padding_mask = model.encode(src)\n",
    "        outputs = []\n",
    "        for batch_idx in range(B):\n",
    "            enc_output = encoder_output[batch_idx:batch_idx + 1]  # (1, T1, n_embd)\n",
    "            src_mask = src_padding_mask[batch_idx:batch_idx + 1]  # (1, T1)\n",
    "            beams = [(torch.ones(1, 1, dtype=torch.long, device=device) * sos_token, 0.0)]\n",
    "            for _ in range(max_len):\n",
    "                new_beams = []\n",
    "                for seq, log_prob in beams:\n",
    "                    if seq[0, -1] == eos_token:\n",
    "                        new_beams.append((seq, log_prob))\n",
    "                        continue\n",
    "                    logits = model.decode(seq, enc_output, src_mask)\n",
    "                    logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    topk_probs, topk_indices = torch.topk(probs, beam_width)\n",
    "                    for i in range(beam_width):\n",
    "                        token_id = topk_indices[0, i].item()\n",
    "                        log_prob = topk_probs[0, i].item()\n",
    "                        new_seq = torch.cat([seq, torch.tensor([[token_id]], device=device)], dim=1)\n",
    "                        new_log_prob = log_prob + log_prob\n",
    "                        new_beams.append((new_seq, new_log_prob))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "                if all(seq[0, -1].item() == eos_token for seq, _ in beams):\n",
    "                    break\n",
    "            best_seq = max(beams, key=lambda x: x[1])[0].squeeze(0)\n",
    "            outputs.append(best_seq)\n",
    "    model.train()\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def translate(model, seqs, max_len, device, en_stoi, zh_itos, sos_token, eos_token, method='beam_search', **kwargs):\n",
    "    B = len(seqs)\n",
    "    inputs = torch.zeros(B, max_len, dtype=torch.long, device=device)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        tokens = tokenize_en(seq)\n",
    "        encoded = encode(tokens, en_stoi)\n",
    "        length = min(len(encoded), max_len)\n",
    "        inputs[i, :length] = torch.tensor(encoded[:length], dtype=torch.long, device=device)\n",
    "    if method == 'top_k_sample':\n",
    "        outputs = top_k(model, inputs, sos_token, eos_token, max_len=max_len, device=device, **kwargs)\n",
    "    elif method == 'beam_search':\n",
    "        outputs = beam_search(model, inputs, sos_token, eos_token, max_len=max_len, device=device, **kwargs)\n",
    "    results = []\n",
    "    for i in range(B):\n",
    "        result = decode(outputs[i], zh_itos)\n",
    "        result = result.replace('<eos>', '').replace('|', '').replace('<sos>', '')\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa countries and europe countries, africa countries and europe countries.\n",
      "paris as the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
      "====================================================================================================\n",
      "method: top_k_sample, top_k: 5, temperature: 0.5\n",
      "非洲国家和欧洲国家非洲国家和欧洲国家.\n",
      "巴黎随着经济危机的深化和不断加深,世界一直在寻找历史相似性来帮助我们理解已经发生的事情.\n"
     ]
    }
   ],
   "source": [
    "seqs = [\n",
    "    \"africa countries and europe countries, africa countries and europe countries.\",\n",
    "    \"paris as the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"\n",
    "]\n",
    "print(\"\\n\".join(seqs))\n",
    "\n",
    "\n",
    "print(\"=\" * 100)\n",
    "args = [{\n",
    "    \"method\": \"top_k_sample\",\n",
    "    \"top_k\": 5,\n",
    "    \"temperature\": 0.5\n",
    "},\n",
    "]\n",
    "\n",
    "sos_token = zh_stoi['<sos>']\n",
    "eos_token = zh_stoi['<eos>']\n",
    "\n",
    "for arg in args:\n",
    "    print(\", \".join([f\"{k}: {v}\" for k, v in arg.items()]))\n",
    "    print(\"\\n\".join(translate(model, seqs, max_len, device, en_stoi, zh_itos, sos_token, eos_token, **arg)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
