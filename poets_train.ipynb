{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8912abb4-ca9d-47cc-afec-992ab0ea603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad3d991-ec48-4c6b-98e8-d31e821d7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a43a2b-5d21-4356-b2e3-87d511cbe06f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/gushiwen.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the data from the JSON file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/gushiwen.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     data = json.load(f)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/gushiwen.json'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('data/gushiwen.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "author_count = defaultdict(int)\n",
    "\n",
    "# Count the occurrences of each author\n",
    "for item in data:\n",
    "    author_count[item['author']] += 1\n",
    "\n",
    "# Sort authors by the number of occurrences\n",
    "sorted_authors = sorted(author_count.items(), key=lambda x: x[1], reverse=True)\n",
    "top_twenty_authors = [item[0] for item in sorted_authors[1:31]]\n",
    "print(top_twenty_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83ec11-1d7a-4f8a-b422-916482a50d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8047\n",
      "\n",
      "各格式诗歌样本:\n",
      "\n",
      "五言绝句样本:\n",
      "  样本1: 床前明月光，疑是地上霜。举头望明月，低头思故乡。\n",
      "  样本2: 危楼高百尺，手可摘星辰。不敢高声语，恐惊天上人。\n",
      "  样本3: 懒摇白羽扇，裸袒青林中。脱巾挂石壁，露顶洒松风。\n",
      "  样本4: 小时不识月，呼作白玉盘。又疑瑶台镜，飞在青云端。\n",
      "\n",
      "七言绝句样本:\n",
      "  样本1: 朝辞白帝彩云间，千里江陵一日还。两岸猿声啼不住，轻舟已过万重山。\n",
      "  样本2: 天门中断楚江开，碧水东流至此回。两岸青山相对出，孤帆一片日边来。\n",
      "  样本3: 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺，疑是银河落九天。\n",
      "  样本4: 李白乘舟将欲行，忽闻岸上踏歌声。桃花潭水深千尺，不及汪伦送我情。\n",
      "\n",
      "五言律诗样本:\n",
      "  样本1: 渡远荆门外，来从楚国游。山随平野尽，江入大荒流。月下飞天镜，云生结海楼。仍怜故乡水，万里送行舟。\n",
      "  样本2: 青山横北郭，白水绕东城。此地一为别，孤蓬万里征。浮云游子意，落日故人情。挥手自兹去，萧萧班马鸣。\n",
      "  样本3: 五月天山雪，无花只有寒。笛中闻折柳，春色未曾看。晓战随金鼓，宵眠抱玉鞍。愿将腰下剑，直为斩楼兰。\n",
      "  样本4: 群峭碧摩天，逍遥不记年。拨云寻古道，倚石听流泉。花暖青牛卧，松高白鹤眠。语来江色暮，独自下寒烟。\n",
      "\n",
      "七言律诗样本:\n",
      "\n",
      "其他样本:\n",
      "\n",
      "古诗格式分布:\n",
      "五言绝句: 734 首 (9.12%)\n",
      "七言绝句: 2763 首 (34.34%)\n",
      "五言律诗: 4550 首 (56.54%)\n",
      "七言律诗: 0 首 (0.00%)\n",
      "其他: 0 首 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "filtered = list(filter(lambda x: x['author'] in top_twenty_authors, data))\n",
    "filtered = list(filter(lambda x: '(' not in x['content'] and ')' not in x['content'], filtered))\n",
    "contents = [item['content'].replace(\"<br/>\", \"\").strip() for item in filtered]\n",
    "contents = list(filter(lambda x: len(x) in [24, 32, 48] and all(c in '，。' or '\\u4e00' <= c <= '\\u9fa5' for c in x), contents))\n",
    "print(len(contents))\n",
    "contents[:10]\n",
    "\n",
    "# 统计古诗格式分布\n",
    "poem_formats = {\n",
    "    \"五言绝句\": 0,  # 20个字左右（4行，每行5字）\n",
    "    \"七言绝句\": 0,  # 28个字左右（4行，每行7字）\n",
    "    \"五言律诗\": 0,  # 40个字左右（8行，每行5字）\n",
    "    \"七言律诗\": 0,  # 56个字左右（8行，每行7字）\n",
    "    \"其他\": 0\n",
    "}\n",
    "# 创建字典来存储每种格式的诗歌样本\n",
    "poem_samples = {\n",
    "    \"五言绝句\": [],\n",
    "    \"七言绝句\": [],\n",
    "    \"五言律诗\": [],\n",
    "    \"七言律诗\": [],\n",
    "    \"其他\": []\n",
    "}\n",
    "\n",
    "for content in contents:\n",
    "    length = len(content)\n",
    "    # 考虑标点符号，实际字数会少一些\n",
    "    if length == 24:  # 五言绝句（考虑可能的变体）\n",
    "        poem_formats[\"五言绝句\"] += 1\n",
    "        poem_samples[\"五言绝句\"].append(content)\n",
    "    elif length == 32:  # 七言绝句\n",
    "        poem_formats[\"七言绝句\"] += 1\n",
    "        poem_samples[\"七言绝句\"].append(content)\n",
    "    elif length == 48:  # 五言律诗\n",
    "        poem_formats[\"五言律诗\"] += 1\n",
    "        poem_samples[\"五言律诗\"].append(content)\n",
    "    elif length == 64:  # 七言律诗\n",
    "        poem_formats[\"七言律诗\"] += 1\n",
    "        poem_samples[\"七言律诗\"].append(content)\n",
    "    else:\n",
    "        poem_formats[\"其他\"] += 1\n",
    "        poem_samples[\"其他\"].append(content)\n",
    "\n",
    "# 打印每种格式的两个样本\n",
    "print(\"\\n各格式诗歌样本:\")\n",
    "for format_name, samples in poem_samples.items():\n",
    "    print(f\"\\n{format_name}样本:\")\n",
    "    for i, sample in enumerate(samples[:4]):\n",
    "        print(f\"  样本{i+1}: {sample}\")\n",
    "\n",
    "total = len(contents)\n",
    "percentages = {format_name: count/total*100 for format_name, count in poem_formats.items()}\n",
    "\n",
    "print(\"\\n古诗格式分布:\")\n",
    "for format_name, count in poem_formats.items():\n",
    "    print(f\"{format_name}: {count} 首 ({percentages[format_name]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76880b-6ffa-41c1-a42b-6a52a3a1777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk ratio: 1822/324432 = 0.56%\n",
      "vocab_size: 3450\n",
      "max_length: 51\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "char_freq = defaultdict(int)\n",
    "min_freq = 3\n",
    "# Count the frequency of each character in contents\n",
    "for content in contents:\n",
    "    for char in content:\n",
    "        char_freq[char] += 1\n",
    "vocab = ['<pad>', '<sos>', '<eos>', '<unk>', '<五绝>', '<七绝>', '<五律>']\n",
    "\n",
    "unk_count = 0\n",
    "for char, freq in char_freq.items():\n",
    "    if freq >= min_freq:\n",
    "        vocab.append(char)\n",
    "    else:\n",
    "        unk_count += freq\n",
    "\n",
    "total_count = sum(char_freq.values())\n",
    "unk_ratio = unk_count / total_count if total_count > 0 else 0\n",
    "print(f\"unk ratio: {unk_count}/{total_count} = {unk_ratio * 100:.2f}%\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "max_length = max(len(content) for content in contents) + 3\n",
    "print(\"max_length:\", max_length)\n",
    "stoi = {c: i for i, c in enumerate(vocab)}\n",
    "itos = {i: c for c, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774d0d4-5d38-4088-b5dc-d7b12bc79f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<七绝> |<sos>|  将  |  军  |  作  |  镇  |  古  |  ？  |  洲  |  ，  |  水  |  腻  |  山  |  春  |  节  |  气  |  柔  |  。  |  清  |  夜  |  满  |  城  |  丝  |  管  |  散  |  ，  |  行  |  人  |  不  |  信  |  是  |  边  |  头  |  。  |<eos>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>\n",
      "  X  |  将  |  军  |  作  |  镇  |  古  |  ？  |  洲  |  ，  |  水  |  腻  |  山  |  春  |  节  |  气  |  柔  |  。  |  清  |  夜  |  满  |  城  |  丝  |  管  |  散  |  ，  |  行  |  人  |  不  |  信  |  是  |  边  |  头  |  。  |<eos>|  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  |  X  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def get_format_token(content):\n",
    "    if len(content) == 24:\n",
    "        return '<五绝>'\n",
    "    elif len(content) == 32:\n",
    "        return '<七绝>'\n",
    "    elif len(content) == 48:\n",
    "        return '<五律>'\n",
    "\n",
    "\n",
    "def encode(seq, add_sos=True, add_eos=True, format_token=None):\n",
    "    indices = [stoi[c] if c in stoi else stoi['<unk>'] for c in seq]\n",
    "    if add_sos:\n",
    "        indices = [stoi['<sos>']] + indices\n",
    "    if format_token:\n",
    "        indices = [stoi[format_token]] + indices\n",
    "    if add_eos:\n",
    "        indices = indices + [stoi['<eos>']]\n",
    "    return indices\n",
    "\n",
    "\n",
    "def decode(indices, align=False):\n",
    "    def decode_token(t):\n",
    "        if isinstance(t, int) and t != -1:\n",
    "            return itos[t].replace('<unk>', '？')\n",
    "        elif isinstance(t, torch.Tensor) and t.item() != -1:\n",
    "            return itos[t.item()].replace('<unk>', '？')\n",
    "        else:\n",
    "            return 'X'\n",
    "    result = [decode_token(idx) for idx in indices]\n",
    "    if align:\n",
    "        return \"|\".join([f\"{c:^5}\" for c in result])\n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "def build_dataset(contents):\n",
    "    xs = torch.zeros((len(contents), max_length), dtype=torch.long)\n",
    "    ys = torch.full((len(contents), max_length), dtype=torch.long, fill_value=-1)\n",
    "    for i, content in enumerate(contents):\n",
    "        encoded = encode(content, format_token=get_format_token(content))\n",
    "        xs[i, :len(encoded)] = torch.tensor(encoded)\n",
    "        ys[i, 1:len(encoded)-1] = torch.tensor(encoded[2:])\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# Randomly shuffle the contents\n",
    "random.shuffle(contents)\n",
    "\n",
    "# Split the contents into training and testing sets (80% train, 20% test)\n",
    "split_index = int(len(contents) * 0.9)\n",
    "train_contents = contents[:split_index]\n",
    "test_contents = contents[split_index:]\n",
    "\n",
    "\n",
    "train_xs, train_ys = build_dataset(train_contents)\n",
    "test_xs, test_ys = build_dataset(test_contents)\n",
    "print(decode(train_xs[0], align=True))\n",
    "print(decode(train_ys[0], align=True))\n",
    "train_set = TensorDataset(train_xs, train_ys)\n",
    "test_set = TensorDataset(test_xs, test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ad391-958d-461b-802d-1ebac25393c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.improvement_counter = 0\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score >= self.best_score - self.min_delta:\n",
    "            self.improvement_counter = 0\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.improvement_counter += 1\n",
    "            if self.improvement_counter >= 3:\n",
    "                self.counter = 0  # 多次改善才清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa9dcb-b6f8-4d99-9e13-6bcf06e3f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size)\n",
    "        self.key = nn.Linear(n_embd, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(2048, 2048)))  # 创建下三角掩码\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, n_embd)\n",
    "        B, T = x.shape[0], x.shape[1]\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attn = q @ k.transpose(-2, -1)  # (B, T, T)\n",
    "        attn = attn * (k.size(-1) ** -0.5)\n",
    "        attn = attn.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)  # (B, T, T)\n",
    "\n",
    "        out = attn @ v  # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size) for _ in range(n_head)])\n",
    "        self.fc = nn.Linear(n_head * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attns = [head(x) for head in self.heads]\n",
    "        out = torch.cat(attns, dim=-1)\n",
    "        out = self.dropout(self.fc(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # 扩展维度\n",
    "            nn.GELU(),  # 激活函数\n",
    "            nn.Linear(4 * n_embd, n_embd),  # 恢复维度\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHead(n_embd, head_size, n_head)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, n_layers, vocab_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(2048, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, head_size, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.ln_in = nn.LayerNorm(n_embd)\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, y=None):\n",
    "        # idx: (B, T), y: (B, T)\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.embedding(idx)  # (B, T, n_embd)\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0).repeat(B, 1)  # (B, T)\n",
    "        pos_emb = self.pos_embedding(pos)  # (B, T, n_embd)\n",
    "        x = self.ln_in(tok_emb + pos_emb)\n",
    "        x = self.dropout(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        # logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        logits = F.linear(x, self.embedding.weight)  # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1), ignore_index=-1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7b30d-26e6-45ad-9f2b-426eda0ff663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "n_embd = 128\n",
    "batch_size = 32\n",
    "n_head = 8\n",
    "n_layers = 4\n",
    "model = TransformerLM(n_embd, n_head, n_layers, vocab_size)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, min_delta=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-2)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aae7d4-389b-41b9-9b5b-0f61f98dba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_samples += x.size(0)\n",
    "    model.train()\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ba6c5-60db-41a6-90b4-f4b3929b8b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_avg_loss 14.1331, test_loss 6.7369                                                   \n",
      "epoch 1, train_avg_loss 7.4386, test_loss 5.9915                                                    \n",
      "epoch 2, train_avg_loss 6.4290, test_loss 5.8101                                                    \n",
      "epoch 3, train_avg_loss 5.9807, test_loss 5.7547                                                    \n",
      "epoch 4, train_avg_loss 5.8473, test_loss 5.7183                                                    \n",
      "epoch 5, train_avg_loss 5.7844, test_loss 5.6944                                                    \n",
      "epoch 6, train_avg_loss 5.7422, test_loss 5.6742                                                    \n",
      "epoch 7, train_avg_loss 5.7058, test_loss 5.6409                                                    \n",
      "epoch 8, train_avg_loss 5.6724, test_loss 5.6159                                                    \n",
      "epoch 9, train_avg_loss 5.6383, test_loss 5.5822                                                    \n",
      "epoch 10, train_avg_loss 5.6004, test_loss 5.5459                                                   \n",
      "epoch 11, train_avg_loss 5.5605, test_loss 5.5070                                                   \n",
      "epoch 12, train_avg_loss 5.5194, test_loss 5.4510                                                   \n",
      "epoch 13, train_avg_loss 5.4621, test_loss 5.3624                                                   \n",
      "epoch 14, train_avg_loss 5.3969, test_loss 5.3037                                                   \n",
      "epoch 15, train_avg_loss 5.3409, test_loss 5.2649                                                   \n",
      "epoch 16, train_avg_loss 5.2935, test_loss 5.2289                                                   \n",
      "epoch 17, train_avg_loss 5.2494, test_loss 5.1868                                                   \n",
      "epoch 18, train_avg_loss 5.2095, test_loss 5.1717                                                   \n",
      "epoch 19, train_avg_loss 5.1756, test_loss 5.1293                                                   \n",
      "epoch 20, train_avg_loss 5.1426, test_loss 5.0999                                                   \n",
      "epoch 21, train_avg_loss 5.1083, test_loss 5.0935                                                   \n",
      "epoch 22, train_avg_loss 5.0789, test_loss 5.0448                                                   \n",
      "epoch 23, train_avg_loss 5.0467, test_loss 5.0305                                                   \n",
      "epoch 24, train_avg_loss 5.0201, test_loss 5.0139                                                   \n",
      "epoch 25, train_avg_loss 4.9924, test_loss 4.9723                                                   \n",
      "epoch 26, train_avg_loss 4.9658, test_loss 4.9559                                                   \n",
      "epoch 27, train_avg_loss 4.9384, test_loss 4.9477                                                   \n",
      "epoch 28, train_avg_loss 4.9092, test_loss 4.9254                                                   \n",
      "epoch 29, train_avg_loss 4.8836, test_loss 4.8958                                                   \n",
      "epoch 30, train_avg_loss 4.8574, test_loss 4.8891                                                   \n",
      "epoch 31, train_avg_loss 4.8335, test_loss 4.8747                                                   \n",
      "epoch 32, train_avg_loss 4.8073, test_loss 4.8513                                                   \n",
      "epoch 33, train_avg_loss 4.7827, test_loss 4.8423                                                   \n",
      "epoch 34, train_avg_loss 4.7600, test_loss 4.8197                                                   \n",
      "epoch 35, train_avg_loss 4.7360, test_loss 4.8098                                                   \n",
      "epoch 36, train_avg_loss 4.7144, test_loss 4.7851                                                   \n",
      "epoch 37, train_avg_loss 4.6895, test_loss 4.7875                                                   \n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 38, train_avg_loss 4.6689, test_loss 4.7672                                                   \n",
      "epoch 39, train_avg_loss 4.6473, test_loss 4.7532                                                   \n",
      "epoch 40, train_avg_loss 4.6214, test_loss 4.7368                                                   \n",
      "epoch 41, train_avg_loss 4.6005, test_loss 4.7195                                                   \n",
      "epoch 42, train_avg_loss 4.5785, test_loss 4.7164                                                   \n",
      "epoch 43, train_avg_loss 4.5559, test_loss 4.6960                                                   \n",
      "epoch 44, train_avg_loss 4.5347, test_loss 4.6834                                                   \n",
      "epoch 45, train_avg_loss 4.5126, test_loss 4.6797                                                   \n",
      "epoch 46, train_avg_loss 4.4894, test_loss 4.6563                                                   \n",
      "epoch 47, train_avg_loss 4.4693, test_loss 4.6472                                                   \n",
      "epoch 48, train_avg_loss 4.4528, test_loss 4.6533                                                   \n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 49, train_avg_loss 4.4320, test_loss 4.6282                                                   \n",
      "epoch 50, train_avg_loss 4.4115, test_loss 4.6211                                                   \n",
      "epoch 51, train_avg_loss 4.3913, test_loss 4.6047                                                   \n",
      "epoch 52, train_avg_loss 4.3767, test_loss 4.5978                                                   \n",
      "epoch 53, train_avg_loss 4.3573, test_loss 4.5980                                                   \n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 54, train_avg_loss 4.3404, test_loss 4.5838                                                   \n",
      "epoch 55, train_avg_loss 4.3249, test_loss 4.5841                                                   \n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch 56, train_avg_loss 4.3086, test_loss 4.5637                                                   \n",
      "epoch 57, train_avg_loss 4.2918, test_loss 4.5593                                                   \n",
      "epoch 58, train_avg_loss 4.2749, test_loss 4.5589                                                   \n",
      "EarlyStopping counter: 3 out of 5\n",
      "epoch 59, train_avg_loss 4.2629, test_loss 4.5472                                                   \n",
      "epoch 60, train_avg_loss 4.2487, test_loss 4.5458                                                   \n",
      "epoch 61, train_avg_loss 4.2332, test_loss 4.5334                                                   \n",
      "epoch 62, train_avg_loss 4.2176, test_loss 4.5319                                                   \n",
      "epoch 63, train_avg_loss 4.2049, test_loss 4.5371                                                   \n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 64, train_avg_loss 4.1926, test_loss 4.5188                                                   \n",
      "epoch 65, train_avg_loss 4.1762, test_loss 4.5172                                                   \n",
      "epoch 66, train_avg_loss 4.1628, test_loss 4.5175                                                   \n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch 67, train_avg_loss 4.1530, test_loss 4.5152                                                   \n",
      "epoch 68, train_avg_loss 4.1386, test_loss 4.5046                                                   \n",
      "epoch 69, train_avg_loss 4.1264, test_loss 4.5073                                                   \n",
      "EarlyStopping counter: 3 out of 5\n",
      "epoch 70, train_avg_loss 4.1185, test_loss 4.4974                                                   \n",
      "epoch 71, train_avg_loss 4.1034, test_loss 4.4922                                                   \n",
      "epoch 72, train_avg_loss 4.0919, test_loss 4.4942                                                   \n",
      "EarlyStopping counter: 4 out of 5\n",
      "epoch 73, train_avg_loss 4.0809, test_loss 4.4914                                                   \n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping at epoch 73 with test loss 4.49137820960572\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "max_epoch = 300\n",
    "\n",
    "\n",
    "def print_progress(epoch, i, train_loader, loss):\n",
    "    progress = (i + 1) / len(train_loader) * 100\n",
    "    bar_length = 30  # Length of the progress bar\n",
    "    filled_length = int(bar_length * progress // 100)\n",
    "    bar = '━' * filled_length + '─' * (bar_length - filled_length)\n",
    "    print(f\"epoch {epoch}, progress |{bar}| {progress:.0f}%, loss {loss.item()}\", end='\\r')\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    i = 0\n",
    "    epoch_losses = []\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % (len(train_loader) // 10) == 0:\n",
    "            print_progress(epoch, i, train_loader, loss)\n",
    "        epoch_losses.append(loss.item())\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "    print(f\"\\r{' '* 100}\", end=\"\\r\")  # 先清除当前行\n",
    "    print(f\"epoch {epoch}, train_avg_loss {avg_loss:.4f}, test_loss {test_loss:.4f}\")\n",
    "    early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping at epoch\", epoch, \"with test loss\", test_loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bb22b-f8e9-4246-b057-04e106d56cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: 0.6\n",
      "<五绝><sos>春眠春水好，独倚洞房开。莫作山僧去，无人不见来。\n",
      "<五绝><sos>春眠秋月夜，晚与一声中。自有人间事，不知不是风。\n",
      "<五绝><sos>春眠千万仞，秋色一声清。应是长生事，不闻青琐行。\n",
      "<五绝><sos>春眠花落尽，日暮雨凄凄。不是风尘泪，何人醉一壶。\n",
      "<五绝><sos>春眠春欲尽，春色复凄凄。不见春风里，何人到此时。\n",
      "<五绝><sos>春眠花下绿，春色夜初晴。只是南山路，今朝入郭城。\n",
      "<五绝><sos>春眠一声声，夜坐一声清。何事秋山客，长歌不可惊。\n",
      "<五绝><sos>春眠风景好，风露晚凄凄。莫道春犹落，空斋不可携。\n",
      "<五绝><sos>春眠风景好，欲与雨声空。不是无人事，偏嫌一夜风。\n",
      "<五绝><sos>春眠不可见，昨夜不成时。今夜更长夜，今宵却到时。\n",
      "<五绝><sos>春眠千里别，暮雨一相随。更有东风雪，长吟一夜时。\n",
      "<五绝><sos>春眠无限意，花落似花时。莫道人间事，无人莫是诗。\n",
      "<五绝><sos>春眠一杯酒，晓色几人行。欲问长安在，还闻此处行。\n",
      "<五绝><sos>春眠新雨后，夜坐一回环。何处风吹角，今宵不解携。\n",
      "<五绝><sos>春眠秋气急，夜月夜同倾。谁是秋风里，人间夜宿行。\n",
      "<五绝><sos>春眠红叶低，风雪绿苔垂。欲问三杯酒，时时一觉时。\n",
      "<五绝><sos>春眠无限意，不解是人间。不是花前事，无妨得此闲。\n",
      "<五绝><sos>春眠春不尽，独宿不同游。欲问南山客，还闻此路愁。\n",
      "<五绝><sos>春眠花满地，秋色满江城。今日无人赏，时时是此行。\n",
      "<五绝><sos>春眠不可见，日暮更纷纷。犹有山中月，何如水上云。\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, max_new_tokens, context, temperature=1.0, top_k=20):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(context)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # 只保留概率最高的top_k个选项\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        input_next = torch.multinomial(probs, num_samples=1)\n",
    "        # 检查是否所有批次都生成了结束标记\n",
    "        if (input_next[:, 0] == stoi['<eos>']).all():\n",
    "            break\n",
    "        context = torch.cat((context, input_next), dim=-1)\n",
    "    model.train()\n",
    "    return context\n",
    "\n",
    "\n",
    "def sample(text, count, model, max_new_tokens, temperature=1.0, format_token=None):\n",
    "    context = torch.tensor(encode(text, add_sos=True, add_eos=False, format_token=format_token), dtype=torch.long)\n",
    "    context = context.unsqueeze(0).repeat(count, 1)\n",
    "    idx = generate(model, max_new_tokens, context, temperature)\n",
    "    result = []\n",
    "    for i in idx:\n",
    "        eos_idx = (i == stoi['<eos>']).nonzero()\n",
    "        if eos_idx.numel() > 0:\n",
    "            i = i[:eos_idx[0]]\n",
    "        result.append(decode(i))\n",
    "    return result\n",
    "\n",
    "\n",
    "for temperature in [0.6]:\n",
    "    print(f\"temperature: {temperature}\")\n",
    "    print(\"\\n\".join(sample(\"春眠\", 20, model, 100, temperature=temperature, format_token='<五绝>')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
