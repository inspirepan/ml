{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load txt：\n",
      "- data/gigaword_input_300000.txt\n",
      "- data/gigaword_summary_300000.txt\n"
     ]
    }
   ],
   "source": [
    "data_size = 300000\n",
    "vocab_size = 150000\n",
    "\n",
    "os.environ['DATA_SIZE'] = '300000'\n",
    "os.environ['VOCAB_SIZE'] = '150000'\n",
    "input_file = f\"data/gigaword_input_{data_size}.txt\"\n",
    "summary_file = f\"data/gigaword_summary_{data_size}.txt\"\n",
    "input_texts = []\n",
    "summary_texts = []\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"UNK\", \"<unk>\")\n",
    "    text = re.sub(r\"#\\S*#\", \"<num>\", text)\n",
    "    text = re.sub(r\"#\", \"<num>\", text)\n",
    "    text = text.replace(\"-lrb-\", \"(\").replace(\"-rrb-\", \")\").replace(\"-lsb-\", \"[\").replace(\"-rsb-\", \"]\").replace(\"-lcb-\", \"{\").replace(\"-rcb-\", \"}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "if os.path.exists(input_file) and os.path.exists(summary_file):\n",
    "    print(f\"load txt：\")\n",
    "    print(f\"- {input_file}\")\n",
    "    print(f\"- {summary_file}\")\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input:\n",
    "        input_texts = [line.strip() for line in f_input]\n",
    "\n",
    "    with open(summary_file, \"r\", encoding=\"utf-8\") as f_summary:\n",
    "        summary_texts = [line.strip() for line in f_summary]\n",
    "\n",
    "else:\n",
    "    print(\"loading dataset...\")\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    dataset = load_dataset(\"gigaword\", trust_remote_code=True)\n",
    "\n",
    "    def filter_short(example):\n",
    "        return len(example['document'].split()) < 35 and len(example['summary'].split()) < 15\n",
    "\n",
    "    small_dataset = dataset['train'].filter(filter_short).select(range(data_size))\n",
    "\n",
    "    with open(input_file, \"w\", encoding=\"utf-8\") as f_input, \\\n",
    "            open(summary_file, \"w\", encoding=\"utf-8\") as f_summary:\n",
    "\n",
    "        for example in small_dataset:\n",
    "            input_text = preprocess(example['document'])\n",
    "            summary_text = preprocess(example['summary'])\n",
    "            f_input.write(input_text + \"\\n\")\n",
    "            f_summary.write(summary_text + \"\\n\")\n",
    "            input_texts.append(input_text)\n",
    "            summary_texts.append(summary_text)\n",
    "    print(f\"write txt: \")\n",
    "    print(f\"- {input_file}\")\n",
    "    print(f\"- {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_size: 300000\n",
      "vocab_size: 150000\n",
      "\n",
      "\n",
      "\n",
      "BPE 模型已保存到 models/bpe_model_300000.json\n",
      "词汇表大小: 62148\n",
      "词汇表已保存到 models/bpe_vocab_300000.txt\n",
      "原文: australia 's current account deficit shrunk by a record <num> billion dollars ( <num> billion us ) in the june quarter due to soaring commodity prices , figures released monday showed .\n",
      "分词结果: ['australia', \"'\", 's', 'current', 'account', 'deficit', 'shrunk', 'by', 'a', 'record', '<num>', 'billion', 'dollars', '(', '<num>', 'billion', 'us', ')', 'in', 'the', 'june', 'quarter', 'due', 'to', 'soaring', 'commodity', 'prices', ',', 'figures', 'released', 'monday', 'showed', '.']\n"
     ]
    }
   ],
   "source": [
    "%run summary_bpe.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_path = f\"models/bpe_model_{data_size}.json\"\n",
    "\n",
    "\n",
    "def load_bpe_model(model_path):\n",
    "    tokenizer = Tokenizer.from_file(model_path)\n",
    "\n",
    "    tokenizer.pad_token = \"<pad>\"\n",
    "    tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "    tokenizer.bos_token = \"<bos>\"\n",
    "    tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n",
    "\n",
    "    tokenizer.eos_token = \"<eos>\"\n",
    "    tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n",
    "\n",
    "    tokenizer.unk_token = \"<unk>\"\n",
    "    tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = load_bpe_model(loaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, tokenizer):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        src_tokens = [self.tokenizer.bos_token_id] + self.tokenizer.encode(src_text).ids + [self.tokenizer.eos_token_id]\n",
    "        tgt_tokens = [self.tokenizer.bos_token_id] + self.tokenizer.encode(tgt_text).ids + [self.tokenizer.eos_token_id]\n",
    "        return torch.tensor(src_tokens, dtype=torch.long), torch.tensor(tgt_tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(input_texts, summary_texts, tokenizer)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_tokens = [item[0] for item in batch]\n",
    "    tgt_tokens = [item[1] for item in batch]\n",
    "    src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return src_tokens, tgt_tokens\n",
    "\n",
    "\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, masking=True):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size)\n",
    "        self.key = nn.Linear(n_embd, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size)\n",
    "        self.masking = masking\n",
    "        if masking:\n",
    "            self.register_buffer(\"mask\", torch.tril(torch.ones(512, 512)))\n",
    "\n",
    "    def forward(self, x, encoder_output=None, src_padding_mask=None):\n",
    "        # x: (B, T, n_embd)\n",
    "        # encoder_output: (B, T1, n_embd)\n",
    "        # src_padding_mask: (B, T)\n",
    "\n",
    "        B, T = x.shape[0], x.shape[1]\n",
    "\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        if encoder_output is not None:\n",
    "            # CROSS-ATTENTION\n",
    "            # encoder_output: (B, T1, n_embd)\n",
    "            k = self.key(encoder_output)  # (B, T1, head_size)\n",
    "            v = self.value(encoder_output)  # (B, T1, head_size)\n",
    "        else:\n",
    "            # SELF-ATTENTION\n",
    "            k = self.key(x)\n",
    "            v = self.value(x)\n",
    "        attn = q @ k.transpose(-2, -1)  # (B, T, T) or (B, T, T1)\n",
    "        attn = attn * (k.size(-1) ** -0.5)\n",
    "        if self.masking:\n",
    "            attn = attn.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        if src_padding_mask is not None:\n",
    "            if encoder_output is not None:\n",
    "                attn = attn.masked_fill(src_padding_mask.unsqueeze(1), float('-inf'))\n",
    "            else:\n",
    "                attn = attn.masked_fill(src_padding_mask.unsqueeze(1).expand(-1, T, -1), float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn, dim=-1)  # (B, T, T)\n",
    "\n",
    "        out = attn_weights @ v  # (B, T, head_size)\n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, masking=True):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, masking) for _ in range(n_head)])\n",
    "        self.fc = nn.Linear(n_head * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, encoder_output=None, src_padding_mask=None, return_attn=False):\n",
    "        attn_outs = [head(x, encoder_output, src_padding_mask) for head in self.heads]\n",
    "        attns = [out[0] for out in attn_outs]\n",
    "        out = torch.cat(attns, dim=-1)  # (B, T, n_head * head_size)\n",
    "        out = self.dropout(self.fc(out))  # (B, T, n_embd)\n",
    "        if return_attn:\n",
    "            attn_weights = [out[1] for out in attn_outs]\n",
    "            return out, attn_weights\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHead(n_embd, head_size, n_head, masking=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x, src_padding_mask=None, return_attn=False):\n",
    "        if return_attn:\n",
    "            sa_out, attn_weights = self.sa(self.ln1(x), src_padding_mask=src_padding_mask, return_attn=True)\n",
    "        else:\n",
    "            sa_out = self.sa(self.ln1(x), src_padding_mask=src_padding_mask)\n",
    "        x = x + sa_out\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return (x, attn_weights) if return_attn else x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, n_head, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHead(n_embd, head_size, n_head, masking=True)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ca = MultiHead(n_embd, head_size, n_head, masking=False)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_padding_mask=None, return_attn=False):\n",
    "        # x: (B, T, n_embd)\n",
    "        # encoder_output: (B, T1, n_embd)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        if return_attn:\n",
    "            ca_out, attn_weights = self.ca(self.ln2(x), encoder_output, src_padding_mask, return_attn=True)\n",
    "        else:\n",
    "            ca_out = self.ca(self.ln2(x), encoder_output, src_padding_mask)\n",
    "            attn_weights = None\n",
    "        x = x + ca_out\n",
    "        x = x + self.ff(self.ln3(x))\n",
    "        return (x, attn_weights) if return_attn else x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, n_layers, vocab_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(512, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(n_embd, head_size, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(n_embd, head_size, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.ln_in = nn.LayerNorm(n_embd)\n",
    "        self.ln_tgt_in = nn.LayerNorm(n_embd)\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def encode(self, src, return_attn=False):\n",
    "        B, T1 = src.shape\n",
    "        src_padding_mask = (src == 0).to(src.device)  # (B, T1)\n",
    "\n",
    "        # ENCODER\n",
    "        tok_emb = self.token_embedding(src)  # (B, T1, n_embd)\n",
    "        pos = torch.arange(0, T1, device=src.device).unsqueeze(0).repeat(B, 1)  # (B, T1)\n",
    "        pos_emb = self.pos_embedding(pos)  # (B, T1, n_embd)\n",
    "        x = self.ln_in(tok_emb + pos_emb)  # (B, T1, n_embd)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        self_attns = []\n",
    "        # Pass padding mask to each encoder block\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            if return_attn:\n",
    "                x, attn_weights = encoder_block(x, src_padding_mask, return_attn=True)\n",
    "                self_attns.append(attn_weights)\n",
    "            else:\n",
    "                x = encoder_block(x, src_padding_mask)\n",
    "        encoder_output = self.ln_f(x)  # (B, T1, n_embd)\n",
    "        return (encoder_output, src_padding_mask, self_attns) if return_attn else (encoder_output, src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, encoder_output, src_padding_mask, return_attn=False):\n",
    "        B, T2 = tgt.shape\n",
    "        tgt_tok_emb = self.token_embedding(tgt)  # (B, T2, n_embd)\n",
    "        pos = torch.arange(0, T2, device=tgt.device).unsqueeze(0).repeat(B, 1)  # (B, T2)\n",
    "        pos_emb = self.pos_embedding(pos)  # (B, T2, n_embd)\n",
    "        x = self.ln_tgt_in(tgt_tok_emb + pos_emb)  # (B, T2, n_embd)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        cross_attns = []\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            if return_attn:\n",
    "                x, attn_weights = decoder_block(x, encoder_output, src_padding_mask, return_attn=True)\n",
    "                cross_attns.append(attn_weights)\n",
    "            else:\n",
    "                x = decoder_block(x, encoder_output, src_padding_mask)\n",
    "        x = self.ln_f(x)  # (B, T2, n_embd)\n",
    "        logits = F.linear(x, self.token_embedding.weight)  # (B, T2, tgt_vocab_size)\n",
    "        return (logits, cross_attns) if return_attn else logits\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: (B, T1), tgt: (B, T2)\n",
    "        B = src.shape[0]\n",
    "        encoder_output, src_padding_mask = self.encode(src)\n",
    "        decoder_target = torch.cat([tgt[:, 1:], torch.zeros(B, 1, dtype=tgt.dtype, device=tgt.device)], dim=1)\n",
    "        logits = self.decode(tgt, encoder_output, src_padding_mask)  # (B, T2, tgt_vocab_size)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), decoder_target.view(-1), ignore_index=0)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.improvement_counter = 0\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score >= self.best_score - self.min_delta:\n",
    "            self.improvement_counter = 0\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.improvement_counter += 1\n",
    "            if self.improvement_counter >= 3:\n",
    "                self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_length: 270000, test_length: 270000, batch_size: 128\n",
      "lr: 0.001, weight_decay: 0.01\n",
      "n_embd: 128, n_head: 4, n_layers: 6, src_vocab_size: 62148, dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layers = 6\n",
    "dropout = 0.3\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"train_length: {len(train_dataset)}, test_length: {len(train_dataset)}, batch_size: {batch_size}\")\n",
    "print(f\"lr: {lr}, weight_decay: {weight_decay}\")\n",
    "print(f\"n_embd: {n_embd}, n_head: {n_head}, n_layers: {n_layers}, src_vocab_size: {vocab_size}, dropout: {dropout}\")\n",
    "\n",
    "model = Model(n_embd, n_head, n_layers, vocab_size, dropout)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, test_loss, path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'model_config': {\n",
    "            'n_embd': n_embd,\n",
    "            'n_head': n_head,\n",
    "            'n_layers': n_layers,\n",
    "            'vocab_size': vocab_size,\n",
    "            'dropout': dropout\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    # 重建模型\n",
    "    model = Model(**checkpoint['model_config'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将模型状态移动到正确的设备\n",
    "    if device.type == 'cuda':\n",
    "        checkpoint['model_state_dict'] = {k: v.to(device) for k, v in checkpoint['model_state_dict'].items()}\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    en_stoi = checkpoint['en_stoi']\n",
    "    en_itos = checkpoint['en_itos']\n",
    "    zh_stoi = checkpoint['zh_stoi']\n",
    "    zh_itos = checkpoint['zh_itos']\n",
    "    max_len = checkpoint['model_config']['max_len']\n",
    "\n",
    "    return model, checkpoint, en_stoi, en_itos, zh_stoi, zh_itos, max_len\n",
    "\n",
    "\n",
    "def print_progress(epoch, progress, loss, epoch_time):\n",
    "    bar_length = 30  # Length of the progress bar\n",
    "    filled_length = int(bar_length * progress // 100)\n",
    "    bar = '━' * filled_length + '─' * (bar_length - filled_length)\n",
    "    time_str = f\"{epoch_time:.1f}s\" if epoch_time < 60 else f\"{epoch_time/60:.1f}min\"\n",
    "    print(f\"epoch {epoch}, progress |{bar}| {progress:.0f}%, loss {loss.item():.4f}, time: {time_str}\", end='\\r')\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            with autocast(device.type):\n",
    "                logits, loss = model(x, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_samples += x.size(0)\n",
    "    model.train()\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, avg train loss: 8.1116, test loss: 5.6471, lr: 0.00100, epoch time: 3.6min, total time: 3.6min\n",
      "epoch 1, avg train loss: 5.3735, test loss: 4.6496, lr: 0.00100, epoch time: 3.6min, total time: 7.1min\n",
      "epoch 2, avg train loss: 4.6191, test loss: 4.1454, lr: 0.00100, epoch time: 3.7min, total time: 10.8min\n",
      "epoch 3, avg train loss: 4.1612, test loss: 3.7641, lr: 0.00100, epoch time: 3.6min, total time: 14.5min\n",
      "epoch 4, avg train loss: 3.8229, test loss: 3.5306, lr: 0.00100, epoch time: 3.6min, total time: 18.1min\n",
      "epoch 5, avg train loss: 3.5821, test loss: 3.3709, lr: 0.00100, epoch time: 3.7min, total time: 21.8min\n",
      "epoch 6, avg train loss: 3.3991, test loss: 3.2278, lr: 0.00100, epoch time: 3.6min, total time: 25.4min\n",
      "epoch 7, avg train loss: 3.2533, test loss: 3.1117, lr: 0.00100, epoch time: 3.7min, total time: 29.1min\n",
      "epoch 8, avg train loss: 3.1296, test loss: 3.0260, lr: 0.00100, epoch time: 3.7min, total time: 32.8min\n",
      "epoch 9, avg train loss: 3.0249, test loss: 2.9426, lr: 0.00100, epoch time: 3.7min, total time: 36.4min\n",
      "检查点已保存到: checkpoint/summary_checkpoint_epoch_10.pt\n",
      "epoch 10, avg train loss: 2.9342, test loss: 2.8869, lr: 0.00100, epoch time: 3.7min, total time: 40.1min\n",
      "epoch 11, avg train loss: 2.8522, test loss: 2.8565, lr: 0.00100, epoch time: 3.6min, total time: 43.7min\n",
      "epoch 12, avg train loss: 2.7800, test loss: 2.7968, lr: 0.00100, epoch time: 3.7min, total time: 47.4min\n",
      "epoch 13, avg train loss: 2.7158, test loss: 2.7473, lr: 0.00100, epoch time: 3.7min, total time: 51.1min\n",
      "epoch 14, avg train loss: 2.6566, test loss: 2.7068, lr: 0.00100, epoch time: 3.6min, total time: 54.7min\n",
      "epoch 15, avg train loss: 2.6012, test loss: 2.6885, lr: 0.00100, epoch time: 3.7min, total time: 58.4min\n",
      "epoch 16, avg train loss: 2.5522, test loss: 2.6706, lr: 0.00100, epoch time: 3.6min, total time: 62.0min\n",
      "epoch 17, avg train loss: 2.5052, test loss: 2.6419, lr: 0.00100, epoch time: 3.7min, total time: 65.6min\n",
      "epoch 18, avg train loss: 2.4625, test loss: 2.6014, lr: 0.00100, epoch time: 3.7min, total time: 69.4min\n",
      "epoch 19, avg train loss: 2.4233, test loss: 2.5839, lr: 0.00100, epoch time: 3.6min, total time: 73.0min\n",
      "检查点已保存到: checkpoint/summary_checkpoint_epoch_20.pt\n",
      "epoch 20, avg train loss: 2.3844, test loss: 2.5684, lr: 0.00100, epoch time: 3.7min, total time: 76.7min\n",
      "epoch 21, avg train loss: 2.3499, test loss: 2.5407, lr: 0.00100, epoch time: 3.6min, total time: 80.4min\n",
      "epoch 22, avg train loss: 2.3170, test loss: 2.5331, lr: 0.00100, epoch time: 3.6min, total time: 84.0min\n",
      "epoch 23, avg train loss: 2.2836, test loss: 2.5231, lr: 0.00100, epoch time: 3.7min, total time: 87.7min\n",
      "epoch 24, avg train loss: 2.2542, test loss: 2.5076, lr: 0.00100, epoch time: 3.6min, total time: 91.3min\n",
      "epoch 25, avg train loss: 2.2259, test loss: 2.5010, lr: 0.00100, epoch time: 3.7min, total time: 94.9min\n",
      "epoch 26, avg train loss: 2.2001, test loss: 2.4915, lr: 0.00100, epoch time: 3.5min, total time: 98.5min\n",
      "epoch 27, avg train loss: 2.1742, test loss: 2.4779, lr: 0.00100, epoch time: 3.6min, total time: 102.0min\n",
      "epoch 28, avg train loss: 2.1503, test loss: 2.4702, lr: 0.00100, epoch time: 3.6min, total time: 105.6min\n",
      "epoch 29, avg train loss: 2.1255, test loss: 2.4666, lr: 0.00100, epoch time: 3.5min, total time: 109.1min\n",
      "检查点已保存到: checkpoint/summary_checkpoint_epoch_30.pt\n",
      "epoch 30, avg train loss: 2.1037, test loss: 2.4501, lr: 0.00100, epoch time: 3.6min, total time: 112.7min\n",
      "epoch 31, avg train loss: 2.0839, test loss: 2.4694, lr: 0.00100, epoch time: 3.4min, total time: 116.1min\n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch 32, avg train loss: 2.0634, test loss: 2.4560, lr: 0.00050, epoch time: 3.6min, total time: 119.7min\n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch 33, avg train loss: 1.9791, test loss: 2.4129, lr: 0.00050, epoch time: 3.6min, total time: 123.3min\n",
      "epoch 34, avg train loss: 1.9523, test loss: 2.4260, lr: 0.00050, epoch time: 3.5min, total time: 126.8min\n",
      "EarlyStopping counter: 3 out of 5\n",
      "epoch 35, avg train loss: 1.9370, test loss: 2.4199, lr: 0.00025, epoch time: 3.6min, total time: 130.4min\n",
      "EarlyStopping counter: 4 out of 5\n",
      "epoch 36, avg train loss: 1.8898, test loss: 2.4108, lr: 0.00025, epoch time: 3.4min, total time: 133.8min\n",
      "epoch 37, avg train loss: 1.8752, test loss: 2.4103, lr: 0.00025, epoch time: 3.6min, total time: 137.4min\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered. Training has been stopped.\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001, verbose=True)\n",
    "\n",
    "while epoch < max_epoch:\n",
    "    epoch_start_time = time.time()  # 记录每个epoch开始时间\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        with autocast(device.type):\n",
    "            out, loss = model(x, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses.append(loss.item())\n",
    "        if i % (len(train_loader) // 20) == 0 and i > 0:\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            print_progress(epoch, (i + 1) / len(train_loader) * 100, loss, epoch_time)\n",
    "\n",
    "    test_loss = evaluate(model, test_loader, device)\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)  # Calculate average training loss\n",
    "    epoch_time = time.time() - epoch_start_time  # 计算每个epoch耗时\n",
    "    total_time = time.time() - start_time  # 计算总耗时\n",
    "    print(\" \" * 90, end=\"\\r\")\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # 打印训练信息，包含时间\n",
    "    epoch_time_str = f\"{epoch_time:.1f}s\" if epoch_time < 60 else f\"{epoch_time/60:.1f}min\"\n",
    "    total_time_str = f\"{total_time:.1f}s\" if total_time < 60 else f\"{total_time/60:.1f}min\"\n",
    "    print(f\"epoch {epoch}, avg train loss: {avg_train_loss:.4f}, test loss: {test_loss:.4f}, \"\n",
    "          f\"lr: {scheduler.get_last_lr()[0]:.5f}, epoch time: {epoch_time_str}, total time: {total_time_str}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_path = f\"checkpoint/summary_checkpoint_epoch_{epoch+1}.pt\"\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, test_loss, save_path)\n",
    "        print(f\"检查点已保存到: {save_path}\")\n",
    "\n",
    "    early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Training has been stopped.\")\n",
    "        break\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型已保存到: checkpoint_star/summary_checkpoint_300k_128.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"checkpoint_star/summary_checkpoint_300k_128.pt\"\n",
    "save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, test_loss, save_path)\n",
    "print(f\"\\n模型已保存到: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(model, src, sos_token, eos_token, max_len=50, device='cpu', top_k=5, temperature=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = src.to(device)\n",
    "        B, T = src.shape[0], src.shape[1]\n",
    "        encoder_output, src_padding_mask = model.encode(src)\n",
    "        sequences = torch.ones(B, 1, dtype=torch.long, device=device) * sos_token\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = model.decode(sequences, encoder_output, src_padding_mask)\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "            topk_logits, top_k_indices = torch.topk(logits, k=min(top_k, logits.size(-1)))  # (B, k)\n",
    "            probs = F.softmax(topk_logits, dim=-1)  # (B, k)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = torch.gather(top_k_indices, 1, next_token_idx)\n",
    "            sequences = torch.cat([sequences, next_token], dim=1)\n",
    "            if (next_token == eos_token).all():\n",
    "                break\n",
    "    model.train()\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def beam_search(model, src, sos_token, eos_token, beam_width=5, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = src.to(device)\n",
    "        B, T = src.shape[0], src.shape[1]\n",
    "        encoder_output, src_padding_mask = model.encode(src)\n",
    "        outputs = []\n",
    "        for batch_idx in range(B):\n",
    "            enc_output = encoder_output[batch_idx:batch_idx + 1]  # (1, T1, n_embd)\n",
    "            src_mask = src_padding_mask[batch_idx:batch_idx + 1]  # (1, T1)\n",
    "            beams = [(torch.ones(1, 1, dtype=torch.long, device=device) * sos_token, 0.0)]\n",
    "            for _ in range(max_len):\n",
    "                new_beams = []\n",
    "                for seq, log_prob in beams:\n",
    "                    if seq[0, -1] == eos_token:\n",
    "                        new_beams.append((seq, log_prob))\n",
    "                        continue\n",
    "                    logits = model.decode(seq, enc_output, src_mask)\n",
    "                    logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    topk_probs, topk_indices = torch.topk(probs, beam_width)\n",
    "                    for i in range(beam_width):\n",
    "                        token_id = topk_indices[0, i].item()\n",
    "                        log_prob = topk_probs[0, i].item()\n",
    "                        new_seq = torch.cat([seq, torch.tensor([[token_id]], device=device)], dim=1)\n",
    "                        new_log_prob = log_prob + log_prob\n",
    "                        new_beams.append((new_seq, new_log_prob))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "                if all(seq[0, -1].item() == eos_token for seq, _ in beams):\n",
    "                    break\n",
    "            best_seq = max(beams, key=lambda x: x[1])[0].squeeze(0)\n",
    "            outputs.append(best_seq)\n",
    "    model.train()\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def translate(model, seqs, max_len, device, tokenizer, method='beam_search', **kwargs):\n",
    "    B = len(seqs)\n",
    "    inputs = torch.zeros(B, max_len, dtype=torch.long, device=device)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        encoded = [tokenizer.bos_token_id] + tokenizer.encode(seq).ids + [tokenizer.eos_token_id]\n",
    "        length = min(len(encoded), max_len)\n",
    "        inputs[i, :length] = torch.tensor(encoded[:length], dtype=torch.long, device=device)\n",
    "    if method == 'top_k_sample':\n",
    "        outputs = top_k(model, inputs, tokenizer.bos_token_id, tokenizer.eos_token_id, max_len=max_len, device=device, **kwargs)\n",
    "    elif method == 'beam_search':\n",
    "        outputs = beam_search(model, inputs, tokenizer.bos_token_id, tokenizer.eos_token_id, max_len=max_len, device=device, **kwargs)\n",
    "    results = []\n",
    "    for i in range(B):\n",
    "        result = tokenizer.decode(outputs[i].tolist())\n",
    "        result = result.replace('<eos>', '').replace('|', '').replace('<sos>', '')\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libyan leader moamer kadhafi monday promised wide political and economic reforms that he said would see ministries dismantled and oil revenues going directly into the pockets of the people.\n",
      "====================================================================================================\n",
      "method: top_k_sample, top_k: 5, temperature: 0.5\n",
      "kadhafi promises wide political reforms\n"
     ]
    }
   ],
   "source": [
    "seqs = [\n",
    "    \"libyan leader moamer kadhafi monday promised wide political and economic reforms that he said would see ministries dismantled and oil revenues going directly into the pockets of the people.\",\n",
    "]\n",
    "print(\"\\n\".join(seqs))\n",
    "\n",
    "\n",
    "print(\"=\" * 100)\n",
    "args = [{\n",
    "    \"method\": \"top_k_sample\",\n",
    "    \"top_k\": 5,\n",
    "    \"temperature\": 0.5\n",
    "},\n",
    "]\n",
    "\n",
    "\n",
    "for arg in args:\n",
    "    print(\", \".join([f\"{k}: {v}\" for k, v in arg.items()]))\n",
    "    print(\"\\n\".join(translate(model, seqs, 50, device, tokenizer, **arg)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
